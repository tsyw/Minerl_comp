# Minerl_comp

## 比赛

### 数据集

这个比赛的目标是完全使用增强学习的方式来完成游戏中的任务，训练数据包括超过6000万帧的游戏数据。在增强学习中，困难主要在于：稀疏的奖励以及分层有逻辑的策略学习。

数据集包括：
1. Navigation，这个部分包括两个数据集，移动和需要攀爬+移动两部分，也有两种奖励方式，一种是稀疏的奖励，只有到达终点才有奖励，一种是密集的奖励函数，根据目标位置的远近程度加以奖励。
2. Treechop，这个部分是砍树的数据集，每次收集一个木头奖励+1,每个episode收集64个木头。
3. Obtain Item，这部分的数据的目标是获取物品以及生存，包括获取铁镐，熟肉，钻石，床，这些都是一些分层的逻辑，需要完成子任务才能进一步。
4. Survival，这部分的数据没有具体的目标，玩家自由设定自己的目标。
一共四大部分，最终的比赛使用MineRL模拟器中的800万个样本，在一台GPU机器上进行4天的训练，目标是获取钻石。算法可以利用的信息包括科技树以及当前的帧的数据。

挖钻石的奖励除了最后的目标外，还包括一些附属的可以获取奖励的目标。

### 规则

参与者在第一轮中上传训练好的模型，在第二轮中上传代码，在官方的环境和数据集中进行训练，要求不能有人类的专业知识。

### 评估

在第1轮期间，将对收到的提交内容进行评估并排行。在回合结束时重新训练，并且去掉得分明显较低的模型。 

在第2轮中，团队可以提交更多文件，每份文件都将在收到后进行重新训练和评估。每个团队的排行取决于第二轮提交的最高分数。

## Baseline 算法

### Proximal Policy Optimization Algorithms (PPO)

[原始文章](https://arxiv.org/abs/1707.06347)

这个算法采用两个策略，其中一个策略观察另一个策略来决定自己的方向。两个策略的概率分布尽可能接近。在之前的TRPO算法中，采用KL散度来表征两个概率分布的接近程度，但是这种做法的问题在于很难选择优化函数以及惩罚项之间的比值。PPO 算法提出了两种做法，其中之一将两个策略的比值设定为一个裁剪函数，当比值过大或者过小时，设为定值：

$$L^{CLIP}(\theta)=E_t(\min (r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t))$$

其中，$r_t$是两个策略的比值，$A$是奖励函数。

除了上面这种做法外，还可以使用自适应的惩罚系数，当KL散度太大，这个系数就扩大，反之减小。

在实际的算法中，使用神经网络作为概率分布，在上面的式子中增加两项，一项表示熵，一项表示平方损失函数。对于策略的奖励，采用下面的奖励函数：

$$A_t=-V(s_t)+r_t+\gamma r_{t+1}+\cdots+\gamma^{T-t+1}r_{T-1}+\gamma^{T-t}V(s_T)$$

实际算法：每次迭代中，并行的有N个客户端在玩游戏，每个客户端，利用旧的参数的策略收集T个时间戳的数据并计算T个奖励函数的数值。然后根据这些数据优化参数。

运行结果： [参考代码](https://github.com/minerllabs/baselines/blob/master/general/chainerrl/baselines/ppo.py)
实际运行了200个episode还没有看到Reward的显著提高。
